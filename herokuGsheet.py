## Import libraries
import json

import requests
import urllib.request
import csv
import gspread_dataframe as gd
import time
import pandas as pd
import gspread
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from gspread_formatting import *
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import chromedriver_autoinstaller

chromedriver_autoinstaller.install()
# json_url = 'https://raw.githubusercontent.com/sum1raj/heroku-gsheet/main/bil.json'
# f = requests.get(json_url)
# json_name = json.loads(f.text)

i = 0
t = ""
# PATH = 'C:\\bin\\chromedriver.exe'
# gc = gspread.service_account(filename="C:\\bin\\bil.json")
# gc = gspread.service_account(filename=json_name)

json_url = 'https://raw.githubusercontent.com/sum1raj/heroku-gsheet/main/bil.json'
filename = 'credentials.json'
res = requests.get(json_url)
with open(filename, 'w') as f:
    f.write(res.text)  # save the data into a file
gc = gspread.service_account(filename=filename)

sh = gc.open_by_url("https://docs.google.com/spreadsheets/d/12c5L6ArSNBJgMAalhtxdvKfz1mZ5cBUzSqUzADqj1F0/edit#gid=818333264")
worksheet = sh.worksheet("Sheet1")
while True:    # cr√©ation d'une boucle while qui s'executera tant que True == True
        ## Step 1 : Extract the HTML page
    try:
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        # driver = webdriver.Chrome(service=s,chrome_options=options)
        try:
            # driver = webdriver.Chrome(PATH, options=options)
            driver = webdriver.Chrome(options=options)
        except Exception as e:
            print(e)
        print("driver", driver)
        driver.get('https://www.coinglass.com/FundingRate')
        content = driver.page_source
        soup = BeautifulSoup(content,features="html.parser")
        t = str(datetime.now())[:-7]
        print(t, "Here T is")
        driver.quit()
 # print "This report has no legal value"
        #Bitcoin Price extract
        req = urllib.request.Request('https://fr.investing.com/crypto/bitcoin/btc-usd',headers={'User-Agent': 'Mozilla/5.0'})
        webpage = urllib.request.urlopen(req).read()
        soupBTC = BeautifulSoup(webpage,'html.parser')
        price_BTC=float(soupBTC.find('span',attrs={"data-test":"instrument-price-last"}).get_text().replace('.','').replace(',','.'))
        print(price_BTC,"Here I am now")
        ## Step 2 : Extract requested data
        #import of the list of requested currencies
        list_cc=['BTC','ETH','SOL','DOT','BNB','LUNA','XRP','ADA','DOGE','LTC','AVAX','MATIC','AXS','LINK','BCH','SAND','EOS','FIL','ATOM','GALA','FTT','MANA','USDT','XTZ','ETC','FTM','NEAR','CRV','ALGO','SUSHI','BIT','UNI','EGLD','TRX','XLM','1INCH','LRC','DYDX','1000SHIB','ZEC','BTT','ICP','AAVE','CRO','OMG','BSV','SHIB','VET','THETA','ALICE','SRM','YFI','MKR','COMP','SLP','ENJ','RUNE','BAT','GRT','RAY','AR','CHZ','SNX','ONE','SXP','TLM','SPELL','KSM','ENS','XMR','SHIB1000','IOTA','DASH','CHR','STORJ','TRU','NEO','ALT','QTUM','SHIT','HNT','ATLAS','RSR','YGG','FLOW','HBAR','ZRX','GTC','SFP','IOST','BOBA','ANKR','ALPHA','IOTX','C98','CELR','DEFI','KSHIB','LINA','REN','REEF','WAVES','KAVA','AUDIO','BADGER','ZEN','BZRX','KEEP','XEM','KNC','HT','MASK','EXCH','CAKE','ICX','SKL','OKB','BAL','CTK','ZIL','POLIS','YFII','DODO','SC','HOT','ONT','BAND','STX','COTI','BLZ','LIT','BAKE','CVC','DENT','KIN','CTSI','PEOPLE','MID','AGLD','TONCOIN']
        BTC = [t,price_BTC]
        Pred = [t,price_BTC]
        row1=True
        for p in soup.find_all('a',attrs={"href":"/funding/BTC"}):
            if p.get_text()!='BTC' and p.get_text()!='-':
                if row1 :
                    BTC.append(float(p.get_text()[:-1]))  # % removed + conversion into float
                    row1=False
                else :
                    Pred.append(float(p.get_text()[:-1]))
                    row1=True
            elif p.get_text()=='-':
                if row1 :
                    BTC.append(p.get_text())
                    row1=False
                else :
                    Pred.append(p.get_text())
                    row1=True
        simple_list=[BTC]
        for cc in list_cc[1:]:
            Crypto = [t,price_BTC]
            for p in soup.find_all('a',attrs={"href":"/funding/"+cc}):
                if p.get_text()!=cc and p.get_text()!='-':
                    Crypto.append(float(p.get_text()[:-1]))
                elif p.get_text()=='-':
                    Crypto.append(p.get_text())
            simple_list.append(Crypto)
        ## Step 3 : Updating of GoogleSheet files
        df = pd.DataFrame(simple_list,columns=['Time','Price BTC','Binance','Okex','Bybit','FTX','Huobi','Gate','Bitget','CoinEx','Binance Token','Okex Token','Bybit Token','Bitmex Token','Huobi Token','Deribit Token'])
        df.insert(0,'Crypto',list_cc)
        print(len(df),"Length of DF")
        try:
            gd.set_with_dataframe(worksheet, df)
        except Exception as e:
            print(e)
    except Exception:
        print("Sorry network issue on",t)
    i+=1
    time.sleep(300)
